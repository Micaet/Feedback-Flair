{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0caf522",
   "metadata": {},
   "source": [
    "<h1>Part 5: fine-tuning<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6e7d7",
   "metadata": {},
   "source": [
    "**Loading the necessary libraries and setting display settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e908f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import roc_auc_score, fbeta_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import label_binarize, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d070681",
   "metadata": {},
   "source": [
    "**Preparing the data for model training (analogous to the previous notebook)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc21963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv(\"train_final.csv\")\n",
    "\n",
    "# Applying MinMaxScaler, so Logistic Regression converges quicker\n",
    "minmax = MinMaxScaler()\n",
    "df2 = pd.DataFrame(minmax.fit_transform(df.drop(columns=[\"sentiment\"])), columns=df.drop(columns=\"sentiment\").columns)\n",
    "df2 = pd.concat([df2, df[[\"sentiment\"]]], axis=1)\n",
    "\n",
    "# Splitting the data to train and test\n",
    "train_df, test_df = train_test_split(df2, test_size=0.3, random_state=2092)\n",
    "\n",
    "# Feautre and target selection \n",
    "X_train = train_df.drop(columns=['sentiment'])\n",
    "y_train = train_df['sentiment']\n",
    "\n",
    "X_test = test_df.drop(columns=['sentiment'])\n",
    "y_test = test_df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ed720c",
   "metadata": {},
   "source": [
    "**Fine-tuning Logistic Regression** <br>\n",
    "Due to computational limitations, we are unable to apply advanced techniques such as cross-validation. Therefore, we fine-tune the model using basic loops over solvers and penalties. Firstly, we tune over 100 iterations to find out how different penalties and C values affect results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5340e499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With L1, C = 0.01 we get F2 = [0.         0.         0.95880423] on training data, F2 = [0.        0.        0.9593406] on testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With L1, C = 0.1 we get F2 = [0.32020285 0.13037142 0.96195167] on training data, F2 = [0.27996071 0.10611561 0.95960616] on testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With L1, C = 1 we get F2 = [0.60567291 0.45296977 0.96105124] on training data, F2 = [0.51481143 0.41673647 0.95366142] on testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With L1, C = 10 we get F2 = [0.63286015 0.48416146 0.95995554] on training data, F2 = [0.53081305 0.44707049 0.95095301] on testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With L1, C = 100 we get F2 = [0.63575088 0.48675532 0.95978674] on training data, F2 = [0.52944984 0.45008251 0.95055134] on testing data\n",
      "With L2, C = 0.01 we get F2 = [0.01744924 0.025356   0.95949513] on training data, F2 = [0.0112765  0.01960784 0.95977626] on testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With L2, C = 0.1 we get F2 = [0.40591367 0.28528245 0.96349664] on training data, F2 = [0.34480376 0.2636493  0.95944404] on testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With L2, C = 1 we get F2 = [0.6044825  0.46285083 0.96112061] on training data, F2 = [0.50933786 0.42804797 0.95279096] on testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With L2, C = 10 we get F2 = [0.63205357 0.4847308  0.95989238] on training data, F2 = [0.52907654 0.44880264 0.95086193] on testing data\n",
      "With L2, C = 100 we get F2 = [0.6352459  0.48689733 0.95976973] on training data, F2 = [0.52938131 0.45004538 0.95052096] on testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for C in [0.01, 0.1, 1, 10, 100]:\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        C=C,\n",
    "        solver='saga',\n",
    "        max_iter=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds_train = model.predict(X_train)\n",
    "    f2_train = fbeta_score(y_train, preds_train, beta=2, average=None)\n",
    "\n",
    "    preds_test = model.predict(X_test)\n",
    "    f2_test = fbeta_score(y_test, preds_test, beta=2, average=None)\n",
    "\n",
    "    print(f\"With L1, C = {C} we get F2 = {f2_train} on training data, F2 = {f2_test} on testing data\")\n",
    "\n",
    "    results.append({\n",
    "        \"Penalty\": \"L1\",\n",
    "        \"C\": C,\n",
    "        \"F2 Score (Train)\": f2_train,\n",
    "        \"F2 Score (Test)\": f2_test\n",
    "    })\n",
    "\n",
    "for C in [0.01, 0.1, 1, 10, 100]:\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=C,\n",
    "        solver='saga',\n",
    "        max_iter=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds_train = model.predict(X_train)\n",
    "    f2_train = fbeta_score(y_train, preds_train, beta=2, average=None)\n",
    "\n",
    "    preds_test = model.predict(X_test)\n",
    "    f2_test = fbeta_score(y_test, preds_test, beta=2, average=None)\n",
    "\n",
    "    print(f\"With L2, C = {C} we get F2 = {f2_train} on training data, F2 = {f2_test} on testing data\")\n",
    "\n",
    "    results.append({\n",
    "        \"Penalty\": \"L2\",\n",
    "        \"C\": C,\n",
    "        \"F2 Score (Train)\": f2_train,\n",
    "        \"F2 Score (Test)\": f2_test\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c770d1a1",
   "metadata": {},
   "source": [
    "**Displaying the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da19dcfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Penalty</th>\n",
       "      <th>C</th>\n",
       "      <th>F2 Score (Train)</th>\n",
       "      <th>F2 Score (Test)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[0.0, 0.0, 0.958804228946409]</td>\n",
       "      <td>[0.0, 0.0, 0.9593405964396787]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[0.3202028541101545, 0.13037142069400134, 0.9619516710191398]</td>\n",
       "      <td>[0.27996070726915523, 0.10611561016475844, 0.9596061576330996]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>[0.6056729094076655, 0.4529697662783218, 0.9610512384193609]</td>\n",
       "      <td>[0.5148114315542216, 0.4167364717708159, 0.9536614223273915]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L1</td>\n",
       "      <td>10.00</td>\n",
       "      <td>[0.6328601500512765, 0.4841614577815447, 0.9599555389013561]</td>\n",
       "      <td>[0.5308130502330399, 0.4470704900421453, 0.950953010926552]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L1</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[0.6357508762469668, 0.48675531727275934, 0.959786742132326]</td>\n",
       "      <td>[0.5294498381877023, 0.4500825082508251, 0.9505513354569959]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>[0.017449238578680203, 0.025355998214937725, 0.9594951341690374]</td>\n",
       "      <td>[0.011276499774470004, 0.0196078431372549, 0.9597762590202742]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>L2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[0.40591366739960655, 0.28528244851690915, 0.9634966418406703]</td>\n",
       "      <td>[0.3448037589828635, 0.26364929752801, 0.9594440360583433]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>L2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>[0.6044824953648162, 0.4628508334520587, 0.9611206139337063]</td>\n",
       "      <td>[0.5093378607809848, 0.4280479680213191, 0.9527909593464117]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>L2</td>\n",
       "      <td>10.00</td>\n",
       "      <td>[0.6320535666072682, 0.48473080317740513, 0.9598923839316916]</td>\n",
       "      <td>[0.5290765444890558, 0.44880264244426094, 0.9508619253422711]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L2</td>\n",
       "      <td>100.00</td>\n",
       "      <td>[0.6352459016393442, 0.4868973300885268, 0.9597697276992142]</td>\n",
       "      <td>[0.529381309862801, 0.4500453757940764, 0.9505209609597836]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Penalty       C  \\\n",
       "0      L1    0.01   \n",
       "1      L1    0.10   \n",
       "2      L1    1.00   \n",
       "3      L1   10.00   \n",
       "4      L1  100.00   \n",
       "5      L2    0.01   \n",
       "6      L2    0.10   \n",
       "7      L2    1.00   \n",
       "8      L2   10.00   \n",
       "9      L2  100.00   \n",
       "\n",
       "                                                   F2 Score (Train)  \\\n",
       "0                                     [0.0, 0.0, 0.958804228946409]   \n",
       "1     [0.3202028541101545, 0.13037142069400134, 0.9619516710191398]   \n",
       "2      [0.6056729094076655, 0.4529697662783218, 0.9610512384193609]   \n",
       "3      [0.6328601500512765, 0.4841614577815447, 0.9599555389013561]   \n",
       "4      [0.6357508762469668, 0.48675531727275934, 0.959786742132326]   \n",
       "5  [0.017449238578680203, 0.025355998214937725, 0.9594951341690374]   \n",
       "6    [0.40591366739960655, 0.28528244851690915, 0.9634966418406703]   \n",
       "7      [0.6044824953648162, 0.4628508334520587, 0.9611206139337063]   \n",
       "8     [0.6320535666072682, 0.48473080317740513, 0.9598923839316916]   \n",
       "9      [0.6352459016393442, 0.4868973300885268, 0.9597697276992142]   \n",
       "\n",
       "                                                  F2 Score (Test)  \n",
       "0                                  [0.0, 0.0, 0.9593405964396787]  \n",
       "1  [0.27996070726915523, 0.10611561016475844, 0.9596061576330996]  \n",
       "2    [0.5148114315542216, 0.4167364717708159, 0.9536614223273915]  \n",
       "3     [0.5308130502330399, 0.4470704900421453, 0.950953010926552]  \n",
       "4    [0.5294498381877023, 0.4500825082508251, 0.9505513354569959]  \n",
       "5  [0.011276499774470004, 0.0196078431372549, 0.9597762590202742]  \n",
       "6      [0.3448037589828635, 0.26364929752801, 0.9594440360583433]  \n",
       "7    [0.5093378607809848, 0.4280479680213191, 0.9527909593464117]  \n",
       "8   [0.5290765444890558, 0.44880264244426094, 0.9508619253422711]  \n",
       "9     [0.529381309862801, 0.4500453757940764, 0.9505209609597836]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69f7b77",
   "metadata": {},
   "source": [
    "As we can see, small C values, i.e. 0.01 and 0.10 yield inferior results. Let's train the models again on more iterations with C = 1, 10, 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8095c124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With L2, C = 1 we get F2 = [0.64500485 0.49348033 0.96162915] on training data, F2 = [0.54336603 0.45815087 0.95207755] on testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With L2, C = 10 we get F2 = [0.69217336 0.55279352 0.96136378] on training data, F2 = [0.58229127 0.50764103 0.94893311] on testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With L2, C = 100 we get F2 = [0.69871761 0.55856325 0.96113123] on training data, F2 = [0.5879017  0.51357925 0.94852362] on testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With L1, C = 1 we get F2 = [0.66844208 0.49987662 0.96055022] on training data, F2 = [0.57417793 0.46170985 0.95112162] on testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With L1, C = 10 we get F2 = [0.6948848  0.55327655 0.96116468] on training data, F2 = [0.58764187 0.50836446 0.94872375] on testing data\n",
      "With L1, C = 100 we get F2 = [0.69875449 0.55916031 0.96112635] on training data, F2 = [0.5879758  0.51402627 0.94860337] on testing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antoni/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results2 = []\n",
    "\n",
    "for C in [1, 10, 100]:\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        C=C,\n",
    "        solver='saga',\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds_train = model.predict(X_train)\n",
    "    f2_train = fbeta_score(y_train, preds_train, beta=2, average=None)\n",
    "\n",
    "    preds_test = model.predict(X_test)\n",
    "    f2_test = fbeta_score(y_test, preds_test, beta=2, average=None)\n",
    "\n",
    "    print(f\"With L2, C = {C} we get F2 = {f2_train} on training data, F2 = {f2_test} on testing data\")\n",
    "\n",
    "    results2.append({\n",
    "        \"Penalty\": \"L2\",\n",
    "        \"C\": C,\n",
    "        \"F2 Score (Train)\": f2_train,\n",
    "        \"F2 Score (Test)\": f2_test\n",
    "    })\n",
    "\n",
    "for C in [1, 10, 100]:\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        C=C,\n",
    "        solver='saga',\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds_train = model.predict(X_train)\n",
    "    f2_train = fbeta_score(y_train, preds_train, beta=2, average=None)\n",
    "\n",
    "    preds_test = model.predict(X_test)\n",
    "    f2_test = fbeta_score(y_test, preds_test, beta=2, average=None)\n",
    "\n",
    "    print(f\"With L1, C = {C} we get F2 = {f2_train} on training data, F2 = {f2_test} on testing data\")\n",
    "\n",
    "    results2.append({\n",
    "        \"Penalty\": \"L1\",\n",
    "        \"C\": C,\n",
    "        \"F2 Score (Train)\": f2_train,\n",
    "        \"F2 Score (Test)\": f2_test\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03693e92",
   "metadata": {},
   "source": [
    "**Displaying the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ba82b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Penalty</th>\n",
       "      <th>C</th>\n",
       "      <th>F2 Score (Train)</th>\n",
       "      <th>F2 Score (Test)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L2</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6450048496605237, 0.4934803349941694, 0.9616291505612679]</td>\n",
       "      <td>[0.5433660299432111, 0.45815087168470625, 0.9520775541810839]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L2</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.6921733608509287, 0.5527935194520739, 0.9613637803924883]</td>\n",
       "      <td>[0.5822912719464444, 0.5076410339782149, 0.9489331142831846]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L2</td>\n",
       "      <td>100</td>\n",
       "      <td>[0.698717610427991, 0.5585632483081728, 0.961131234915952]</td>\n",
       "      <td>[0.5879017013232514, 0.5135792460478313, 0.948523622047244]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.6684420772303595, 0.49987661719603765, 0.9605502227955466]</td>\n",
       "      <td>[0.5741779250573541, 0.46170985311107443, 0.9511216218868369]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L1</td>\n",
       "      <td>10</td>\n",
       "      <td>[0.6948848023673642, 0.5532765513645055, 0.9611646793371249]</td>\n",
       "      <td>[0.587641866330391, 0.5083644632126035, 0.9487237517958003]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>L1</td>\n",
       "      <td>100</td>\n",
       "      <td>[0.6987544859615791, 0.5591603053435115, 0.9611263509829365]</td>\n",
       "      <td>[0.5879758003529115, 0.5140262688503324, 0.9486033739493317]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Penalty    C                                               F2 Score (Train)  \\\n",
       "0      L2    1   [0.6450048496605237, 0.4934803349941694, 0.9616291505612679]   \n",
       "1      L2   10   [0.6921733608509287, 0.5527935194520739, 0.9613637803924883]   \n",
       "2      L2  100     [0.698717610427991, 0.5585632483081728, 0.961131234915952]   \n",
       "3      L1    1  [0.6684420772303595, 0.49987661719603765, 0.9605502227955466]   \n",
       "4      L1   10   [0.6948848023673642, 0.5532765513645055, 0.9611646793371249]   \n",
       "5      L1  100   [0.6987544859615791, 0.5591603053435115, 0.9611263509829365]   \n",
       "\n",
       "                                                 F2 Score (Test)  \n",
       "0  [0.5433660299432111, 0.45815087168470625, 0.9520775541810839]  \n",
       "1   [0.5822912719464444, 0.5076410339782149, 0.9489331142831846]  \n",
       "2    [0.5879017013232514, 0.5135792460478313, 0.948523622047244]  \n",
       "3  [0.5741779250573541, 0.46170985311107443, 0.9511216218868369]  \n",
       "4    [0.587641866330391, 0.5083644632126035, 0.9487237517958003]  \n",
       "5   [0.5879758003529115, 0.5140262688503324, 0.9486033739493317]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde9221",
   "metadata": {},
   "source": [
    "Logistic Regression with L1 and C=100 proved to be the best model. Let's train it again along XGBoost and try different ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fb538",
   "metadata": {},
   "source": [
    "**Creating ensemble models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab2e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training linear model\n",
      "Finished training linear model\n",
      "F2 for linear model: [0.5879758  0.51402627 0.94860337]\n",
      "Training XGBoost\n",
      "Finished training XGBoost\n",
      "F2 for XGBoost: [0.46598998 0.52673021 0.96063354]\n"
     ]
    }
   ],
   "source": [
    "linear_model = LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        C=100,\n",
    "        solver='saga',\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "print(\"Training linear model\")\n",
    "linear_model.fit(X_train, y_train)\n",
    "print(\"Finished training linear model\")\n",
    "linear_preds = linear_model.predict(X_test)\n",
    "linear_f2 = fbeta_score(y_test, linear_preds, beta=2, average=None)\n",
    "print(f\"F2 for linear model: {linear_f2}\")\n",
    "\n",
    "xg_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "print(\"Training XGBoost\")\n",
    "xg_model.fit(X_train, y_train)\n",
    "print(\"Finished training XGBoost\")\n",
    "xg_preds = xg_model.predict(X_test)\n",
    "xg_f2 = fbeta_score(y_test, xg_preds, beta=2, average=None)\n",
    "print(f\"F2 for XGBoost: {xg_f2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3c70164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "weights_list = [(x, y) for x, y in itertools.product(\n",
    "    [round(i * 0.1, 1) for i in range(1, 10)],\n",
    "    [round(i * 0.1, 1) for i in range(1, 10)]\n",
    ")]\n",
    "\n",
    "proba_lr = linear_model.predict_proba(X_test)\n",
    "proba_xgb = xg_model.predict_proba(X_test)\n",
    "\n",
    "results = []\n",
    "\n",
    "for w_lr, w_xgb in weights_list:\n",
    "\n",
    "    ensemble_proba = w_lr * proba_lr + w_xgb * proba_xgb\n",
    "\n",
    "    y_pred = np.argmax(ensemble_proba, axis=1)\n",
    "\n",
    "    fbeta_each = fbeta_score(y_test, y_pred, beta=2, average=None, zero_division=0)\n",
    "    fbeta_macro = fbeta_score(y_test, y_pred, beta=2, average='macro', zero_division=0)\n",
    "\n",
    "    results.append({\n",
    "        'w_lr': w_lr,\n",
    "        'w_xgb': w_xgb,\n",
    "        'fbeta_0': fbeta_each[0],\n",
    "        'fbeta_1': fbeta_each[1],\n",
    "        'fbeta_2': fbeta_each[2],\n",
    "        'fbeta_macro': fbeta_macro\n",
    "    })\n",
    "\n",
    "res = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab6cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_fbeta0 = res.nlargest(5, 'fbeta_0')\n",
    "top5_fbetamacro = res.nlargest(5, 'fbeta_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be99b861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_lr</th>\n",
       "      <th>w_xgb</th>\n",
       "      <th>fbeta_0</th>\n",
       "      <th>fbeta_1</th>\n",
       "      <th>fbeta_2</th>\n",
       "      <th>fbeta_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.589980</td>\n",
       "      <td>0.517312</td>\n",
       "      <td>0.951058</td>\n",
       "      <td>0.686116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.589944</td>\n",
       "      <td>0.516411</td>\n",
       "      <td>0.951960</td>\n",
       "      <td>0.686105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.589646</td>\n",
       "      <td>0.517199</td>\n",
       "      <td>0.951348</td>\n",
       "      <td>0.686065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.589646</td>\n",
       "      <td>0.517635</td>\n",
       "      <td>0.951137</td>\n",
       "      <td>0.686140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.587193</td>\n",
       "      <td>0.515266</td>\n",
       "      <td>0.953111</td>\n",
       "      <td>0.685190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    w_lr  w_xgb   fbeta_0   fbeta_1   fbeta_2  fbeta_macro\n",
       "72   0.9    0.1  0.589980  0.517312  0.951058     0.686116\n",
       "45   0.6    0.1  0.589944  0.516411  0.951960     0.686105\n",
       "54   0.7    0.1  0.589646  0.517199  0.951348     0.686065\n",
       "63   0.8    0.1  0.589646  0.517635  0.951137     0.686140\n",
       "73   0.9    0.2  0.587193  0.515266  0.953111     0.685190"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_fbeta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50735a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_lr</th>\n",
       "      <th>w_xgb</th>\n",
       "      <th>fbeta_0</th>\n",
       "      <th>fbeta_1</th>\n",
       "      <th>fbeta_2</th>\n",
       "      <th>fbeta_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.589646</td>\n",
       "      <td>0.517635</td>\n",
       "      <td>0.951137</td>\n",
       "      <td>0.686140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.589980</td>\n",
       "      <td>0.517312</td>\n",
       "      <td>0.951058</td>\n",
       "      <td>0.686116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.589944</td>\n",
       "      <td>0.516411</td>\n",
       "      <td>0.951960</td>\n",
       "      <td>0.686105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.589646</td>\n",
       "      <td>0.517199</td>\n",
       "      <td>0.951348</td>\n",
       "      <td>0.686065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.587193</td>\n",
       "      <td>0.515266</td>\n",
       "      <td>0.953111</td>\n",
       "      <td>0.685190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    w_lr  w_xgb   fbeta_0   fbeta_1   fbeta_2  fbeta_macro\n",
       "63   0.8    0.1  0.589646  0.517635  0.951137     0.686140\n",
       "72   0.9    0.1  0.589980  0.517312  0.951058     0.686116\n",
       "45   0.6    0.1  0.589944  0.516411  0.951960     0.686105\n",
       "54   0.7    0.1  0.589646  0.517199  0.951348     0.686065\n",
       "73   0.9    0.2  0.587193  0.515266  0.953111     0.685190"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_fbetamacro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4621217",
   "metadata": {},
   "source": [
    "The chosen weights are [w_lr, w_xgb] = [0.9, 0.1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c25f3c",
   "metadata": {},
   "source": [
    "**Note:** the final ensemble model is trained in `train_model.py`,\n",
    "all its performance metrics are written to `results.txt`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
